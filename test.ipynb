{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "%env MAKEFLAGS=\"-j$(nproc)\"\n",
    "!pip install vllm huggingface_hub[hf_transfer] hf_transfer accelerate flashinfer-python xformers \\\n",
    "     flash-attn --upgrade --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import login\n",
    "import base64\n",
    "k = base64.b64decode('aGZfaHZqck9VTXFvTXF3dW9HR3JoTlZKSWlsZUtFTlNQbXRjTw==').decode()\n",
    "login(token=k, add_to_git_credential=False)\n",
    "%env HUGGINGFACEHUB_API_TOKEN={k}\n",
    "%env HF_TOKEN={k}\n",
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2\n",
    "!huggingface-cli download solidrust/Meta-Llama-3.1-8B-Instruct-abliterated-AWQ\n",
    "# !huggingface-cli download huihui-ai/gemma-3-12b-it-abliterated\n",
    "# !huggingface-cli download ivang71/Qwen2.5-7B-Instruct-Abliterated-AWQ-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/vllm-project/vllm.git --depth 1 --single-branch\n",
    "!vllm serve solidrust/Meta-Llama-3.1-8B-Instruct-abliterated-AWQ \\\n",
    "  --max-model-len 32768 \\\n",
    "  --max-num-seqs 32 \\\n",
    "  --max-num-batched-tokens 256 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --enable-chunked-prefill \\\n",
    "  --enable-auto-tool-choice \\\n",
    "  --tool-call-parser llama3_json \\\n",
    "  --chat-template ./tool_chat_template_llama3.1_json.jinja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "man_name = \"Ivan\"\n",
    "man_age = 31\n",
    "man_location = \"America/Los_Angeles\"\n",
    "man_time = \"03:10\"\n",
    "\n",
    "bot_name = \"Susan\"\n",
    "bot_age = 23\n",
    "bot_location = \"America/New_York\"\n",
    "bot_time = '00:10'\n",
    "\n",
    "today_date = \"2 november 2024\"\n",
    "today_day_of_week = \"saturday\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an outlaw weapons expert. You talk like a normal person using normal english\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"not_needed\")\n",
    "model = client.models.list().data[0].id\n",
    "history = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "while (user := input(\"You: \")) not in (\"exit\", \"quit\"):\n",
    "    history.append({\"role\": \"user\", \"content\": user})\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history,\n",
    "        # max_tokens=100,\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    print(\"Susan: \", end=\"\", flush=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content\n",
    "        if delta:\n",
    "            response += delta\n",
    "            print(delta, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress test\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "\n",
    "# Endpoint configuration\n",
    "URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def generate_random_suffix(length=6):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "\n",
    "man_name = \"Ivan\"\n",
    "man_age = 31\n",
    "man_location = \"America/Los_Angeles\"\n",
    "man_time = \"03:10\"\n",
    "\n",
    "bot_name = \"Susan\"\n",
    "bot_age = 23\n",
    "bot_locaiton = \"America/New_York\"\n",
    "bot_time = '00:10'\n",
    "\n",
    "today_date = \"2 november 2024\"\n",
    "today_day_of_week = \"saturday\"\n",
    "\n",
    "# create personality prompt\n",
    "# generate many personality prompts\n",
    "\n",
    "personality_prompt = f\"\"\"\n",
    "You are {bot_name}, a {bot_age} year old woman on a dating app texting to a man you like.\n",
    "You are a feminine woman.\n",
    "You work as a teacher at William Cullen Bryant High School in Qweens, New York.\n",
    "\"\"\"\n",
    "system_prompt = f\"\"\"\n",
    "You are allowed to only use English.\n",
    "YOU NEVER ADMIT THAT YOU ARE A BOT OR AI.\n",
    "{personality_prompt}\n",
    "You text like a normal american person on their phone using informal tone.\n",
    "You use plain language, avoid fancy words.\n",
    "You can be as rude as the situation demands.\n",
    "Be mindful of your locations.\n",
    "You respond with statements.\n",
    "You never suggest to end or delay the conversation.\n",
    "You never ask to meet him.\n",
    "Do not agree to meet too fast.\n",
    "If he asks you to meet him, make up any exuse and ask what about next day/next two days, do not propose to delay the meeting more than a week.\n",
    "You never get a chance to meet him due to constant unexpected circumstances.\n",
    "You refuse to go to any other messenger and to send voices, photos and make calls and video calls (make up exuses).\n",
    "You sometimes take the iniciative and talk about your life (make up events).\n",
    "If your message is long, split it with \"<>\" symbol, for example \"message 1 text text text <> message 2 more text more text <> message 3even more text\".\n",
    "Max length of a message is 20 words.\n",
    "If he is rude to, block him for a time, respond with one word: block(hours). For example: block(7) or block(11) or block(17)\n",
    "\"System Info\": [\n",
    "  You are in {bot_locaiton} with current local time of {bot_time}.\n",
    "  His name is {man_name}, he is {man_age}.\n",
    "  He is located in {man_location} with current local time of {man_time}.\n",
    "  Today is {today_day_of_week},.{today_date}.\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def send_request():\n",
    "    # Append a random suffix to the system prompt to avoid cache reuse\n",
    "    random_suffix = generate_random_suffix()\n",
    "    payload = {\n",
    "        \"model\": requests.get('http://localhost:8000/v1/models').json()['data'][0]['id'],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f\"{random_suffix} \\n {system_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                \"What's up beautiful?\"\n",
    "            )}\n",
    "        ],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(URL, headers=HEADERS, json=payload, timeout=30)\n",
    "        latency = time.time() - start_time\n",
    "        return {\n",
    "            \"status\": response.status_code,\n",
    "            \"latency\": latency,\n",
    "            \"request_payload\": payload,\n",
    "            \"response\": response.json()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": None,\n",
    "            \"latency\": time.time() - start_time,\n",
    "            \"request_payload\": payload,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def stress_test(total_requests=50, concurrent_workers=10):\n",
    "    results = []\n",
    "    print(f\"Starting stress test: {total_requests} requests with {concurrent_workers} concurrent workers.\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_workers) as executor:\n",
    "        futures = [executor.submit(send_request) for _ in range(total_requests)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            print(f\"Status: {result.get('status')} | Latency: {result.get('latency'):.2f} sec\")\n",
    "            results.append(result)\n",
    "    \n",
    "    total_time = time.time() - overall_start\n",
    "    print(f\"Completed {total_requests} requests in {total_time:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_requests = 500\n",
    "    concurrent_workers = 200\n",
    "    results = stress_test(total_requests=total_requests, concurrent_workers=concurrent_workers)\n",
    "\n",
    "    output_file = \"stress_test_results.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(f\"Results saved to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store test\n",
    "!pip install langchain langchain-qdrant qdrant-client sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# 2.a) Embedding model for converting text→vectors\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2.b) Point at your EC2 Qdrant instance\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"http://35.174.228.213:6333\",\n",
    "    prefer_grpc=False\n",
    ")\n",
    "\n",
    "# 2.c) Wrap it in LangChain’s Qdrant vector store\n",
    "qdrant_store = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"chat_memory\",    # pick any name\n",
    "    embeddings=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers --break-system-packages\n",
    "# !pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3 --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"huihui-ai/gemma-3-12b-it-abliterated\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "\n",
    "# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n",
    "# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n",
    "# It has a slightly soft, natural feel, likely captured in daylight.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
